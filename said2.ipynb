{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import*\n",
    "import json\n",
    "import glob \n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = \"mongodb://bouman:80um4N!@ec2-15-188-255-64.eu-west-3.compute.amazonaws.com:27017/\"\n",
    "client = MongoClient(connection)\n",
    "db = client.get_database('media_analysis')\n",
    "col = db[\"articles\"]\n",
    "fr_news = col.find({'meta.source.language': 'fr'},{\"_id\":0,\"title\": 1,\"text\":1,\"date\":1}).limit(5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=fr_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Jamal Khashoggi, critique du régime saoudien a...\n",
       "1    Coronavirus: plus de 1.200 nouveaux morts aux ...\n",
       "2    Les concentrations de différents polluants rel...\n",
       "3    Vous avez aimé la bataille hivernale entre les...\n",
       "4    Le premier facteur demeure le salaire. Plus d’...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_texts = df[\"text\"]\n",
    "fr_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"months.json\", 'r') as f:\n",
    "    months = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent', 'janvier', 'février', 'mars', 'avril', 'mai', 'juin', 'juillet', 'août', 'septembre', 'octobre', 'novembre', 'décembre', 'lundi', 'mardi', 'jeudi', 'mercredi', 'vendredi', 'samedi', 'dimanche']\n"
     ]
    }
   ],
   "source": [
    "stopwords_fr = stopwords.words(\"french\")\n",
    "stopwords_nl = stopwords.words(\"dutch\")\n",
    "stopwords_fr += months\n",
    "print(stopwords_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    word_tokens = word_tokenize(texts)\n",
    "    filtered_sentence = [w for w in word_tokens if w.lower() not in stopwords_fr]\n",
    "    final_text = \" \".join(filtered_sentence)\n",
    "    return final_text\n",
    "\n",
    "df[\"fr_texts_processed\"] = fr_texts.map(lambda x: remove_stopwords(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_texts_processed = df[\"fr_texts_processed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>fr_texts_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-22 04:49:17</td>\n",
       "      <td>Jamal Khashoggi, critique du régime saoudien a...</td>\n",
       "      <td>La famille du journaliste saoudien tué dans le...</td>\n",
       "      <td>Jamal Khashoggi , critique régime saoudien apr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-22 04:32:51</td>\n",
       "      <td>Coronavirus: plus de 1.200 nouveaux morts aux ...</td>\n",
       "      <td>Coronavirus: plus de 1.200 nouveaux morts aux ...</td>\n",
       "      <td>Coronavirus : plus 1.200 nouveaux morts États-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-22 04:22:52</td>\n",
       "      <td>Les concentrations de différents polluants rel...</td>\n",
       "      <td>La qualité de l’air en nette amélioration à Br...</td>\n",
       "      <td>concentrations différents polluants relevées 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-22 04:00:00</td>\n",
       "      <td>Vous avez aimé la bataille hivernale entre les...</td>\n",
       "      <td>Open VLD: une élection présidentielle à coutea...</td>\n",
       "      <td>aimé bataille hivernale entre libéraux francop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-22 04:17:10</td>\n",
       "      <td>Le premier facteur demeure le salaire. Plus d’...</td>\n",
       "      <td>Six ménages sur dix ont perdu de l’argent dura...</td>\n",
       "      <td>premier facteur demeure salaire . Plus ’ tiers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-05-22 04:58:27</td>\n",
       "      <td>Le ciel sera plus nuageux ce matin si ce n’est...</td>\n",
       "      <td>Météo de ce vendredi : des nuages plus présents</td>\n",
       "      <td>ciel plus nuageux matin si ’ côté Gaume sud ’ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-05-21 19:27:46</td>\n",
       "      <td>Face à la hausse des hospitalisations, on est ...</td>\n",
       "      <td>«L’erreur est humaine. Certes, mais on oublie ...</td>\n",
       "      <td>Face hausse hospitalisations , davantage ’ inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-05-22 04:00:00</td>\n",
       "      <td>Les pouvoirs locaux souffrent des conséquences...</td>\n",
       "      <td>Les communes paient la note de la crise sanitaire</td>\n",
       "      <td>pouvoirs locaux souffrent conséquences ’ épidé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-05-22 04:47:39</td>\n",
       "      <td>Un tracteur et sa remorque se sont retournés s...</td>\n",
       "      <td>Mobilinfo de ce vendredi : Un tracteur et sa r...</td>\n",
       "      <td>tracteur remorque retournés N621 Soumagne , ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-05-21 17:39:24</td>\n",
       "      <td>Magnette, Rousseau et Bouchez se sont rencontr...</td>\n",
       "      <td>Gouvernement fédéral: le duo Magnette-Rousseau...</td>\n",
       "      <td>Magnette , Rousseau Bouchez rencontrés expliqu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                                               text  \\\n",
       "0 2020-05-22 04:49:17  Jamal Khashoggi, critique du régime saoudien a...   \n",
       "1 2020-05-22 04:32:51  Coronavirus: plus de 1.200 nouveaux morts aux ...   \n",
       "2 2020-05-22 04:22:52  Les concentrations de différents polluants rel...   \n",
       "3 2020-05-22 04:00:00  Vous avez aimé la bataille hivernale entre les...   \n",
       "4 2020-05-22 04:17:10  Le premier facteur demeure le salaire. Plus d’...   \n",
       "5 2020-05-22 04:58:27  Le ciel sera plus nuageux ce matin si ce n’est...   \n",
       "6 2020-05-21 19:27:46  Face à la hausse des hospitalisations, on est ...   \n",
       "7 2020-05-22 04:00:00  Les pouvoirs locaux souffrent des conséquences...   \n",
       "8 2020-05-22 04:47:39  Un tracteur et sa remorque se sont retournés s...   \n",
       "9 2020-05-21 17:39:24  Magnette, Rousseau et Bouchez se sont rencontr...   \n",
       "\n",
       "                                               title  \\\n",
       "0  La famille du journaliste saoudien tué dans le...   \n",
       "1  Coronavirus: plus de 1.200 nouveaux morts aux ...   \n",
       "2  La qualité de l’air en nette amélioration à Br...   \n",
       "3  Open VLD: une élection présidentielle à coutea...   \n",
       "4  Six ménages sur dix ont perdu de l’argent dura...   \n",
       "5    Météo de ce vendredi : des nuages plus présents   \n",
       "6  «L’erreur est humaine. Certes, mais on oublie ...   \n",
       "7  Les communes paient la note de la crise sanitaire   \n",
       "8  Mobilinfo de ce vendredi : Un tracteur et sa r...   \n",
       "9  Gouvernement fédéral: le duo Magnette-Rousseau...   \n",
       "\n",
       "                                  fr_texts_processed  \n",
       "0  Jamal Khashoggi , critique régime saoudien apr...  \n",
       "1  Coronavirus : plus 1.200 nouveaux morts États-...  \n",
       "2  concentrations différents polluants relevées 1...  \n",
       "3  aimé bataille hivernale entre libéraux francop...  \n",
       "4  premier facteur demeure salaire . Plus ’ tiers...  \n",
       "5  ciel plus nuageux matin si ’ côté Gaume sud ’ ...  \n",
       "6  Face hausse hospitalisations , davantage ’ inc...  \n",
       "7  pouvoirs locaux souffrent conséquences ’ épidé...  \n",
       "8  tracteur remorque retournés N621 Soumagne , ch...  \n",
       "9  Magnette , Rousseau Bouchez rencontrés expliqu...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39m# texts_out.append(final)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m(tokens)\n\u001b[0;32m---> 14\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m fr_texts_processed\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m x: lemmatization(x))\n\u001b[1;32m     15\u001b[0m \u001b[39m# lemmatized_texts = lemmatization(remove_stopwords(fr_texts))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# print(lemmatized_texts[0][0:90])\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/pandas/core/series.py:4397\u001b[0m, in \u001b[0;36mSeries.map\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4318\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\n\u001b[1;32m   4319\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4320\u001b[0m     arg: Callable \u001b[39m|\u001b[39m Mapping \u001b[39m|\u001b[39m Series,\n\u001b[1;32m   4321\u001b[0m     na_action: Literal[\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   4322\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[1;32m   4323\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4324\u001b[0m \u001b[39m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[1;32m   4325\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4395\u001b[0m \u001b[39m    dtype: object\u001b[39;00m\n\u001b[1;32m   4396\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4397\u001b[0m     new_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_values(arg, na_action\u001b[39m=\u001b[39;49mna_action)\n\u001b[1;32m   4398\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_values, index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39m__finalize__(\n\u001b[1;32m   4399\u001b[0m         \u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4400\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/pandas/core/base.py:924\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    923\u001b[0m \u001b[39m# mapper is a function\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m new_values \u001b[39m=\u001b[39m map_f(values, mapper)\n\u001b[1;32m    926\u001b[0m \u001b[39mreturn\u001b[39;00m new_values\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[54], line 14\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39m# texts_out.append(final)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m(tokens)\n\u001b[0;32m---> 14\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m fr_texts_processed\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: lemmatization(x))\n\u001b[1;32m     15\u001b[0m \u001b[39m# lemmatized_texts = lemmatization(remove_stopwords(fr_texts))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# print(lemmatized_texts[0][0:90])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m, in \u001b[0;36mlemmatization\u001b[0;34m(text, allowed_postags)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatization\u001b[39m(text, allowed_postags\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mNOUN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mADJ\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVERB\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mADV\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m----> 2\u001b[0m     nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mfr_core_news_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m     \u001b[39m# texts_out = []\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m# for text in texts:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     52\u001b[0m         name,\n\u001b[1;32m     53\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     54\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     55\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     56\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     57\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/util.py:465\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[1;32m    464\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/util.py:501\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 501\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, enable\u001b[39m=\u001b[39;49menable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/fr_core_news_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moverrides)\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m    681\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[0;32m--> 682\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    683\u001b[0m     data_path,\n\u001b[1;32m    684\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    685\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    686\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    687\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m    688\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    689\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    690\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/util.py:547\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    538\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[1;32m    539\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[1;32m    540\u001b[0m     config,\n\u001b[1;32m    541\u001b[0m     vocab\u001b[39m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     meta\u001b[39m=\u001b[39mmeta,\n\u001b[1;32m    546\u001b[0m )\n\u001b[0;32m--> 547\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39;49mfrom_disk(model_path, exclude\u001b[39m=\u001b[39;49mexclude, overrides\u001b[39m=\u001b[39;49moverrides)\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/language.py:2156\u001b[0m, in \u001b[0;36mLanguage.from_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (path \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mexists() \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m     \u001b[39m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m     exclude \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(exclude) \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m-> 2156\u001b[0m util\u001b[39m.\u001b[39;49mfrom_disk(path, deserializers, exclude)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m path  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_link_components()\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/util.py:1392\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39mfor\u001b[39;00m key, reader \u001b[39min\u001b[39;00m readers\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1390\u001b[0m     \u001b[39m# Split to support file names like meta.json\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m     \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:\n\u001b[0;32m-> 1392\u001b[0m         reader(path \u001b[39m/\u001b[39;49m key)\n\u001b[1;32m   1393\u001b[0m \u001b[39mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/language.py:2150\u001b[0m, in \u001b[0;36mLanguage.from_disk.<locals>.<lambda>\u001b[0;34m(p, proc)\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mfrom_disk\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2149\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m-> 2150\u001b[0m     deserializers[name] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m p, proc\u001b[39m=\u001b[39mproc: proc\u001b[39m.\u001b[39;49mfrom_disk(  \u001b[39m# type: ignore[misc]\u001b[39;49;00m\n\u001b[1;32m   2151\u001b[0m         p, exclude\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mvocab\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2152\u001b[0m     )\n\u001b[1;32m   2153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (path \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mexists() \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m     \u001b[39m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m     exclude \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(exclude) \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:303\u001b[0m, in \u001b[0;36mLemmatizer.from_disk\u001b[0;34m(self, path, exclude)\u001b[0m\n\u001b[1;32m    301\u001b[0m deserialize[\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m p: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mfrom_disk(p, exclude\u001b[39m=\u001b[39mexclude)\n\u001b[1;32m    302\u001b[0m deserialize[\u001b[39m\"\u001b[39m\u001b[39mlookups\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m p: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookups\u001b[39m.\u001b[39mfrom_disk(p)\n\u001b[0;32m--> 303\u001b[0m util\u001b[39m.\u001b[39;49mfrom_disk(path, deserialize, exclude)\n\u001b[1;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_tables()\n\u001b[1;32m    305\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/util.py:1392\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39mfor\u001b[39;00m key, reader \u001b[39min\u001b[39;00m readers\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1390\u001b[0m     \u001b[39m# Split to support file names like meta.json\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m     \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:\n\u001b[0;32m-> 1392\u001b[0m         reader(path \u001b[39m/\u001b[39;49m key)\n\u001b[1;32m   1393\u001b[0m \u001b[39mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:302\u001b[0m, in \u001b[0;36mLemmatizer.from_disk.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    300\u001b[0m deserialize: Dict[\u001b[39mstr\u001b[39m, Callable[[Any], Any]] \u001b[39m=\u001b[39m {}\n\u001b[1;32m    301\u001b[0m deserialize[\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m p: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mfrom_disk(p, exclude\u001b[39m=\u001b[39mexclude)\n\u001b[0;32m--> 302\u001b[0m deserialize[\u001b[39m\"\u001b[39m\u001b[39mlookups\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m p: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlookups\u001b[39m.\u001b[39;49mfrom_disk(p)\n\u001b[1;32m    303\u001b[0m util\u001b[39m.\u001b[39mfrom_disk(path, deserialize, exclude)\n\u001b[1;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_tables()\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/lookups.py:313\u001b[0m, in \u001b[0;36mLookups.from_disk\u001b[0;34m(self, path, filename, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[39mwith\u001b[39;00m filepath\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file_:\n\u001b[1;32m    312\u001b[0m         data \u001b[39m=\u001b[39m file_\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_bytes(data)\n\u001b[1;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/lookups.py:276\u001b[0m, in \u001b[0;36mLookups.from_bytes\u001b[0;34m(self, bytes_data, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the lookups from a bytestring.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mbytes_data (bytes): The data to load.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mDOCS: https://spacy.io/api/lookups#from_bytes\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tables \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 276\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m srsly\u001b[39m.\u001b[39;49mmsgpack_loads(bytes_data)\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tables[key] \u001b[39m=\u001b[39m Table(key, value)\n\u001b[1;32m    278\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/srsly/_msgpack_api.py:27\u001b[0m, in \u001b[0;36mmsgpack_loads\u001b[0;34m(data, use_list)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m# msgpack-python docs suggest disabling gc before unpacking large messages\u001b[39;00m\n\u001b[1;32m     26\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[0;32m---> 27\u001b[0m msg \u001b[39m=\u001b[39m msgpack\u001b[39m.\u001b[39;49mloads(data, raw\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, use_list\u001b[39m=\u001b[39;49muse_list)\n\u001b[1;32m     28\u001b[0m gc\u001b[39m.\u001b[39menable()\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m msg\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/srsly/msgpack/__init__.py:79\u001b[0m, in \u001b[0;36munpackb\u001b[0;34m(packed, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         object_hook \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(decoder, chain\u001b[39m=\u001b[39mobject_hook)\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mobject_hook\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m object_hook\n\u001b[0;32m---> 79\u001b[0m \u001b[39mreturn\u001b[39;00m _unpackb(packed, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/srsly/msgpack/_unpacker.pyx:191\u001b[0m, in \u001b[0;36msrsly.msgpack._unpacker.unpackb\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/srsly/msgpack/_msgpack_numpy.py:65\u001b[0m, in \u001b[0;36mdecode_numpy\u001b[0;34m(obj, chain)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(x)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_numpy\u001b[39m(obj, chain\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m    Decoder for deserializing numpy data types.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def lemmatization(text, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    # texts_out = []\n",
    "    # for text in texts:\n",
    "    doc = nlp(text)\n",
    "    tokens = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ in allowed_postags:\n",
    "            tokens += \" \"\n",
    "            tokens += token.lemma_.lower()\n",
    "    # texts_out.append(final)\n",
    "    return(tokens)\n",
    "\n",
    "df[\"test\"] = fr_texts_processed.map(lambda x: lemmatization(x))\n",
    "# lemmatized_texts = lemmatization(remove_stopwords(fr_texts))\n",
    "# print(lemmatized_texts[0][0:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lemmatization(df[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m \u001b[39m# df[\"text\"][0]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m, in \u001b[0;36mlemmatization\u001b[0;34m(text, allowed_postags)\u001b[0m\n\u001b[1;32m      2\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mfr_core_news_sm\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mner\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[39m# texts_out = []\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# for text in texts:\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m      6\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc:\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/language.py:1030\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1010\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1011\u001b[0m     text: Union[\u001b[39mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     component_cfg: Optional[Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1015\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1030\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_doc(text)\n\u001b[1;32m   1031\u001b[0m     \u001b[39mif\u001b[39;00m component_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m         component_cfg \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Desktop/capstone_project/venv/lib/python3.11/site-packages/spacy/language.py:1124\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1123\u001b[0m     \u001b[39mreturn\u001b[39;00m Doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\u001b[39m.\u001b[39mfrom_bytes(doc_like)\n\u001b[0;32m-> 1124\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE1041\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(doc_like)))\n",
      "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "lemmatization(df[\"text\"])\n",
    "# df[\"text\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
